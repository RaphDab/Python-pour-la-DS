{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Récupération, pré-traitement et analyse de posts Reddit en rapport avec l'entreprise Apple afin de conduire une analyse de sentiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cette partie est de **récupérer des posts Reddit** relatifs à Apple afin de les **analyser** et de pouvoir les incorporer à notre **algorithme de trading final** sous forme d'une aide à la décision.\n",
    "\n",
    "On va d'abord **importer** naïvement les derniers posts relatifs à Apple à l'aide de l'**API de Reddit**. On va ensuite trier ces données, les nettoyer et faire un ensemble de **statistiques descriptives** sur les posts nettoyés. Nous allons d'abord observer un nuage de mots sur les titres des derniers posts. Puis, nous créerons un dataframe regroupant les derniers posts, leur auteur, leur date de publication, leur titre ainsi que leur URL.\n",
    "\n",
    "Par suite, nous observerons le lien qui peut exister entre le nombre de posts publiés et le cours d'Apple en bourse.\n",
    "\n",
    "Puis nous déterminerons l'influence de certains auteurs dans le nombre de posts écrits.\n",
    "\n",
    "Enfin, nous réaliserons des statistiques descriptives sur le dataframe créé initialment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, on installe **praw** et **wordcloud** qui seront deux bibliothèques essentielles pour la suite de ce notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install praw wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe aussi toutes les **bibliothèques** qui nous seront utiles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud,  STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe également les fonctions codées dans les fichiers précédents et qui nous seront à nouveau utiles ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    #Passage du texte en miniscules\n",
    "    text=text.lower()\n",
    "    #Suppression des chiffres\n",
    "    text=re.sub(r'\\d+', '', text)\n",
    "    #Suppresion de /r/\n",
    "    text =re.sub(r'/r/', '', text)\n",
    "    #Suppression de la ponctuation et des symboles spéciaux\n",
    "    text=re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def count_common_words(text):\n",
    "    words=text.split()\n",
    "    dict={}\n",
    "    for el in words :\n",
    "        if el not in dict:\n",
    "            dict[el]=1\n",
    "        else:\n",
    "            dict[el]+=1\n",
    "    return dict\n",
    "\n",
    "def most_common_words(dictionary):\n",
    "    sorted_dict=dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True))\n",
    "    return sorted_dict\n",
    "\n",
    "\n",
    "\n",
    "def combine_dictionaries(df,df_column_name):\n",
    "    combined_dict={}\n",
    "    column_index=df.columns.get_loc(str(df_column_name))\n",
    "    for i in range(len(df)):\n",
    "        temp_dictionary=df.iloc[i,int(column_index)]\n",
    "\n",
    "        for key, value in temp_dictionary.items():\n",
    "            if key not in combined_dict:\n",
    "                combined_dict[key]=value\n",
    "            else:\n",
    "                combined_dict[key]+=value\n",
    "    return combined_dict\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def clean_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_text=[word for word in words if word not in stops]\n",
    "    return ' '.join(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **I/ Récupération des articles sous forme d'un dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons d'abord observer les mots les plus utilisés dans les titres des 500 derniers posts relatifs à Apple. Puis, nous allons créer et nettoyer un dataframe regroupant les informations principales concernants les posts concernant Apple. Pour ce faire, nous utilisons l'API de Reddit. Nous avons,préalablement, créé un profit \"programmeur\" sur Reddit. Ce profit permet d'obtenir l'identification client et le mot de passe, qui donnent la possibilité d'importer les données de Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ Nuage de mots des 500 derniers posts sur Apple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait les configurations nécessaires à l'utilisation de PRAW pour la récupération des posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='xIq0ALkJ0RWzM5ZLwwiQKA',\n",
    "                     client_secret='DeHliktGK8nfhDXsJFiebqgeZKhHXQ',\n",
    "                     user_agent='Matlpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère les 500 premiers posts du subreddit 'apple' :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('apple')\n",
    "top_posts = subreddit.new(limit=500) #On récupère ainsi les 500 premiers posts Reddit sur Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'un dataframe \n",
    "posts_data = []\n",
    "for post in top_posts:\n",
    "    text = post.selftext if post.selftext else \"Text Not Available\"\n",
    "    date = pd.to_datetime(post.created_utc, unit='s')\n",
    "    post_data = {\n",
    "        \"Titre\": post.title,\n",
    "        \"Auteur\": str(post.author),\n",
    "        \"Texte\": text,\n",
    "        \"Date\": date,\n",
    "        \"url\": post.url\n",
    "    }\n",
    "    posts_data.append(post_data)\n",
    "\n",
    "df = pd.DataFrame(posts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On nettoie le texte récupéré :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_text']=df['Texte'].apply(cleaning_text)\n",
    "df['Cleaned_text']=df['Cleaned_text'].apply(clean_stopwords)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=' '.join(df['Cleaned_text'])\n",
    "words=text.split()\n",
    "word_counts=Counter(words)\n",
    "wordcloud = WordCloud(width = 800, height = 500, background_color ='white').generate_from_frequencies(word_counts)\n",
    "\n",
    "plt.figure(figsize = (8, 8)) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"**Advice thread**\" revient très souvent. En effet, on remarque en analysant le dataframe, qu'un user publie **le même post** chaque jour avec à chaque fois ce titre. Nous pouvons l'enlever. \n",
    "Par ailleurs, on constate que de nombreux articles ne contiennent pas de texte. Ces derniers contiennent généralement un **lien** vers un article de presse extérieur à Reddit. Afin de récupérer du contenu plus intéressant, nous allons récupérer les **commentaires principaux** sous chacun des posts récupérés. Ce sont eux qui seront analysés par notre modèle de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des commentaires principaux :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On supprime les posts avec un \"text not available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text_cleaned'].str.lower() != \"text not available\"]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère les commentaires principaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_submission_id(url):\n",
    "    # Utilisation d'une expression régulière pour extraire l'ID de la soumission\n",
    "    match = re.search(r'/comments/(\\w+)/', url)\n",
    "    # Si l'expression régulière trouve un match, retourner l'ID, sinon retourner None\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_top_comments(url):\n",
    "    submission_id = extract_submission_id(url)\n",
    "    \n",
    "    # Vérifier si submission_id est valide\n",
    "    if not submission_id:\n",
    "        print(f\"L'ID de la soumission extrait de l'URL '{url}' est invalide.\")\n",
    "        return []\n",
    "    \n",
    "    # Si submission_id est valide, continuer à récupérer les commentaires\n",
    "    submission = reddit.submission(id=submission_id)\n",
    "    submission.comment_sort = 'top'\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    \n",
    "    top_comments = []\n",
    "    for comment in submission.comments[:5]:  # Prendre les 5 premiers commentaires\n",
    "        top_comments.append(comment.body)\n",
    "    \n",
    "    return top_comments\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    top_comments = get_top_comments(row['url'])\n",
    "    for i, comment in enumerate(top_comments):\n",
    "        df.loc[index, f'comment_{i+1}'] = comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On nettoie le texte des commentaires principaux récupérés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_1_clean'] = df['comment_1'].apply(cleaning_text)\n",
    "df['comment_2_clean'] = df['comment_2'].apply(cleaning_text)\n",
    "df['comment_3_clean'] = df['comment_3'].apply(cleaning_text)\n",
    "df['comment_4_clean'] = df['comment_4'].apply(cleaning_text)\n",
    "df['comment_5_clean'] = df['comment_5'].apply(cleaning_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On supprime les posts \"Advice Thread\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['text_cleaned'].str.contains(\"daily advice thread\", case=False, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse descriptive du dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir obtenu un dataframe nettoyé regroupant les 500 derniers posts Reddit relatifs à Apple, nous allors désormais utiliser ce dataframe pour tenter d'observer un lien entre le cours boursier d'Apple et le nombre de posts publiés. Puis nous observerons l'impact qu'ont certains utilisateurs sur le nombre de posts publiés. Enfin, nous réaliserons des statistiques descriptives sur les posts Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/ Lien entre le cours d'Apple et le nombre de posts écrits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons observer s'il existe, a priori, un lien entre le nombre de posts publiés sur Reddit et relatifs à Apple et le cours d'Apple en bourse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous allons tracer un graphe regroupant, en abscisse, le temps et, en ordonnée, le nombre de posts cumulé publiés sur Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = 1  #Cette colonne sert à compter les occurences\n",
    "df_grouped = df.groupby(df['Date'].dt.date)['count'].sum().cumsum()\n",
    "\n",
    "#On crée le graphique\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_grouped.index, df_grouped.values, marker='o', linestyle='-')\n",
    "plt.title('Cumulative Number of Posts Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Number of Posts')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons une courbe **quasi linéaire**, avec quelques fluctuations. Nous allons observer si ces fluctuations peuvent avoir un **lien** avec le cours d'Apple en bourse.\n",
    "\n",
    "Superposons la courbe obtenue précédemment avec **la courbe de l'indice d'Apple en bourse**. Ainsi, nous pourrons observer s'il peut y avoir un lien entre l'indice d'Apple en bourse et le nombre de posts écrits sur Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Trouver la date du post le plus ancien\n",
    "oldest_post_date = df['date'].min()\n",
    "\n",
    "# Utiliser cette date pour télécharger les données historiques d'Apple\n",
    "apple_ticker = 'AAPL'\n",
    "apple_data = yf.download(apple_ticker, start=oldest_post_date)\n",
    "\n",
    "df_apple_aligned = apple_data.reindex(df['date'].unique(), method='ffill')\n",
    "\n",
    "\n",
    "# Créer le graphe avec deux axes y\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Axe pour le nombre de posts\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative Number of Posts', color='tab:blue')\n",
    "ax1.plot(df_grouped.index, df_grouped.values, marker='o', linestyle='-')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Instantier un second axe y qui partage le même axe x\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Stock Price', color='tab:red')\n",
    "ax2.plot(df_apple_aligned.index, df_apple_aligned['Close'], color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Titre et style\n",
    "plt.title('Cumulative Number of Apple-related Posts Over Time vs Apple Stock Price')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphiquement, nous n'observons **pas de lien direct évident** entre le nombre de posts publiés et le cours d'Apple. Néanmoins, il semblerait qu'un plus grand nombre de posts soient écrits juste avant un choc du cours boursier d'Apple.\n",
    "\n",
    "Nous allons maintenant **calculer le coefficient de correlation**, au sens de Pearson, entre le nombre de posts Reddit relatifs à Apple publiés et le cours boursier d'Apple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On réindexe le dataframe\n",
    "df_apple_aligned.reset_index(inplace=True)\n",
    "df_apple_aligned.rename(columns={'index': 'Date'}, inplace=True)\n",
    "df_apple_aligned\n",
    "# On extrait de df_apple_aligned un dataframe composé d'une colonne avec la date et le cours moyen d'Apple à cette date\n",
    "\n",
    "# On convertit la colonne 'datetime' en type datetime\n",
    "df_apple_aligned['Date'] = pd.to_datetime(df_apple_aligned['Date'])\n",
    "\n",
    "# On extrait la date sans l'heure\n",
    "df_apple_aligned['date2'] = df_apple_aligned['Date'].dt.date\n",
    "\n",
    "# On groupe par date et calculer la moyenne du cours boursier\n",
    "daily_avg_stock_data = df_apple_aligned.groupby('date2')['Close'].mean().reset_index()\n",
    "\n",
    "# On fait de même pour df_sorted\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# On extrait la date sans l'heure\n",
    "df['date2'] = df['date'].dt.date\n",
    "\n",
    "# On groupe par date et calculer la moyenne du cours boursier\n",
    "daily_post_count= df.groupby('date2')['count'].count().reset_index()\n",
    "\n",
    "# On calcule le coefficient de correlation\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# On fusionne les deux ensembles de données sur la date\n",
    "merged_data = pd.merge(daily_post_count, daily_avg_stock_data, on='date2')\n",
    "\n",
    "# On calcule la corrélation de Pearson\n",
    "correlation, p_value = pearsonr(merged_data['count'], merged_data['Close'])\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que le coefficient de correlation est **beaucoup plus faible**, en valeur absolu que 1. Donc le nombre de post publiés sur Reddit ne semble que **peu corrélé** au cours boursier. En réalité, ce sont peut-être les sentiments dégagés dans ces posts et dans les commentaires qui peuvent être corrélés avec le cours boursier d'Apple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B/Lien entre les auteurs des posts et le nombre de posts écrits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant déterminer **les plus gros contributeurs sur Reddit**. Pour ce faire, nous allons d'abord créer un dataframe avec l'ensemble des contributeurs du /r/Apple. Puis nous réaliserons des statistiques sur ces contributeurs, comme leur influence sur l'ensemble des posts Reddit publiés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'abord, crée un dataframe regroupant l'ensemble des contributeurs du subreddit /r/Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors = df.groupby('auteur').agg(\n",
    "    number_of_posts=pd.NamedAgg(column='Titre', aggfunc='count'),\n",
    "    latest_post_date=pd.NamedAgg(column='date', aggfunc='max')\n",
    ").reset_index()\n",
    "\n",
    "# Tri des auteurs par le nombre d'articles écrits, en ordre décroissant\n",
    "df_authors_sorted = df_authors.sort_values(by='number_of_posts', ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_authors_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que *favicondotico* et *AutoModerator* sont les plus gros contributeurs du lien Reddit sur Apple. Nous allons observer l'influence de ces contributeurs dans le nombre de posts publiés. Nous allons d'abord extraire de sorted un Dataframe avec seulement les posts de *favicondotico* et de *AutoModerator*. Puis nous allons tracer sur un même graphe le nombre de posts, en cumulé, de *AutoModerator* et de *favicondotico*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_favicondotico = df[(df['auteur'] == 'favicondotico') | (df['auteur'] == 'AutoModerator')]\n",
    "df_favicondotico = df_favicondotico.reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df['cumulative_posts'] = df['count'].cumsum()\n",
    "\n",
    "# Pour df_favicondotico_reset, nous faisons la même chose\n",
    "df_favicondotico['date'] = pd.to_datetime(df_favicondotico['date'])\n",
    "df_favicondotico.set_index('date', inplace=True)\n",
    "df_favicondotico['cumulative_posts_favicondotico'] = df_favicondotico['count'].cumsum()\n",
    "\n",
    "# Création du graphique\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Tracer le nombre cumulatif de tous les posts\n",
    "ax1.plot(df.index, df['cumulative_posts'], label='Cumulative Posts (All Authors)', color='tab:blue')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative Posts (All Authors)', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Créer un second axe pour le nombre cumulatif de posts par favicondotico\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_favicondotico.index, df_favicondotico['cumulative_posts_favicondotico'], label='Cumulative Posts (favicondotico)', color='tab:red')\n",
    "ax2.set_ylabel('Cumulative Posts (favicondotico, AutoModerator)', color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Légendes et titre\n",
    "fig.tight_layout()\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.title('Cumulative Posts Over Time')\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df_favicondotico.reset_index(inplace=True)\n",
    "\n",
    "df.rename(columns={'index': 'date'}, inplace=True)\n",
    "df_favicondotico.rename(columns={'index': 'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que le nombre de posts publiés par *favicondotico* et *AutoModerator* a **une très grande influence** sur le nombre de posts publiés au total. En effet, la tendance du nombre de posts publiés sur le subreddit /r/Apple est la même que la tendance du nombre de posts publiés par *favicondotico* ou *AutoModerator*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons visualiser la proportion de participation aux posts de chaque contributeur à l'aide d'un **diagramme en camembert**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le total des posts\n",
    "total_posts = df_authors_sorted['number_of_posts'].sum()\n",
    "\n",
    "# Calculer le pourcentage des posts totaux pour chaque auteur\n",
    "df_authors_sorted['percentage'] = (df_authors_sorted['number_of_posts'] / total_posts) * 100\n",
    "\n",
    "# Filtrer pour inclure seulement les auteurs avec plus de 1% des posts\n",
    "df_filtered = df_authors_sorted[df_authors_sorted['percentage'] > 1]\n",
    "\n",
    "# Couleurs en nuances de jaune\n",
    "colors = ['gold', 'yellow', 'lightyellow', 'lemonchiffon', 'khaki', 'darkkhaki', 'palegoldenrod']\n",
    "\n",
    "# Tracer le diagramme en camembert\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(df_filtered['percentage'], labels=df_filtered['auteur'], autopct='%1.1f%%',colors=colors, startangle=140)\n",
    "plt.axis('equal')  # S'assurer que le camembert est bien circulaire\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que *favicondotico* et *AutoModerator* contribuent **à plus de la moitié** des posts complets. D'où la grande influence que ces deux contributeurs ont sur l'ensemble des posts publiés. Nous comprenons désormais pourquoi la tendance concernant le nombre total de posts publiés est très proche de celle concernant le nombre total de posts publiés par *favicondotico* et *AutoModerator*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C/ Statistiques sur les mots les plus utilisés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cette partie est de réaliser des **statistiques descriptives sur l'occurence de certains mots** dans les textes écrits dans les dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En partie II/A nous avons réalisé un nuage de mots permettant de rendre compte des mots les plus utilisés dans les titres des articles. On va maintenant créer un dataframe qui compte l'occurence des mots identifiés dans la section II/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ajoute le texte compilé des 5 commentaires les plus appréciés sur chaque post et on nettoie ce texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_comments_text_join(url, limit=5):\n",
    "    # Utiliser la fonction extract_submission_id pour obtenir l'identifiant du post\n",
    "    submission_id = extract_submission_id(url)\n",
    "    # Vérifier que submission_id n'est pas None\n",
    "    if submission_id:\n",
    "        submission = reddit.submission(id=submission_id)\n",
    "\n",
    "        # Récupère et trie les commentaires par score\n",
    "        submission.comment_sort = 'top'\n",
    "        submission.comments.replace_more(limit=5)  # Charge tous les commentaires\n",
    "        top_comments = list(submission.comments[:limit])\n",
    "\n",
    "        # Compile le texte des commentaires\n",
    "        comments_text = ' '.join(comment.body for comment in top_comments if hasattr(comment, 'body'))\n",
    "        return comments_text\n",
    "    else:\n",
    "        return \"No valid ID found in URL\"\n",
    "# Ajouter une nouvelle colonne avec le texte des commentaires compilés\n",
    "df['top_comments_text_compiled'] = df['url'].apply(get_top_comments_text_join)\n",
    "df['top_comments_clean_compiled']=df['top_comments_text_compiled'].apply(cleaning_text)\n",
    "\n",
    "# On supprime aussi les colonnes \"Count\" et \"Cumulative_count\"\n",
    "df = df.drop('count', axis=1)\n",
    "df = df.drop('cumulative_count', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons désormais réaliser une analyse descriptive du dataframe. On va déterminer l'occurence des mots observés dans le nuage de mots précédent. Pour ce faire nous allons créer une colonne \"count_mot\" pour connaitre l'occurence d'un mot dans le post principal et une colonne \"count_mot2\" pour connaitre son occurence dans les commentaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['apple_count'] = df['Cleaned_text'].str.lower().str.count('apple')\n",
    "df['iphone_count'] = df['Cleaned_text'].str.lower().str.count('iphone')\n",
    "df['airpods_count'] = df['Cleaned_text'].str.lower().str.count('airpods')\n",
    "df['iphonepro_count'] = df['Cleaned_text'].str.lower().str.count('iphone pro')\n",
    "df['android_count'] = df['Cleaned_text'].str.lower().str.count('android')\n",
    "df['ios_count'] = df['Cleaned_text'].str.lower().str.count('ios')\n",
    "df['ipad_count'] = df['Cleaned_text'].str.lower().str.count('ipad')\n",
    "df['ipadpro_count'] = df['Cleaned_text'].str.lower().str.count('ipad pro')\n",
    "df['samsung_count'] = df['Cleaned_text'].str.lower().str.count('samsung')\n",
    "\n",
    "df['apple_count2'] = df['top_comments_clean_compiled'].str.lower().str.count('apple')\n",
    "df['iphone_count2'] = df['top_comments_clean_compiled'].str.lower().str.count('iphone')\n",
    "df['airpods_count2'] = df['top_comments_clean_compiled'].str.lower().str.count('airpods')\n",
    "df['iphonepro_count2'] = df['top_comments_clean_compiled'].str.lower().str.count('iphone pro')\n",
    "df['android_count2'] = df['top_comments_clean_compiled'].str.lower().str.count('android')\n",
    "df['ios_count2'] = df['top_comments_clean_compiled'].str.lower().str.count('ios')\n",
    "df['ipad_count2'] = df['top_comments_clean_compiled'].str.lower().str.count('ipad')\n",
    "df['ipadpro_count2'] = df['top_comments_clean_compiled'].str.lower().str.count('ipad pro')\n",
    "df['samsung_count2'] = df['top_comments_clean_compiled'].str.lower().str.count('samsung')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous supprimons également tous les textes non valides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['top_comments_cleaned'].str.lower() != 'no valid id found in url']\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons la fonction \"describe\" pour obtenir un tableau relatif aux statistiques sur l'occurence des mots sélectionnés plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df.describe(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Au total, le mot \"Apple\" n'apparait que peu de fois par article. Cela est sûrement dû au fait que les contributeurs parlent régulièrement des produits d'Apple, et non pas d'Apple même. En effet, les contributeurs semblent plus aborder le thème des produits comme l'Iphone, l'Ipad, les airpods que d'Apple en lui-même. On remarque que le mot \"Android\" est très rarement utilisé également. Ainsi, les posts sur Apple ne parle que peu du système d'exploitation concurrent à IOS. Une limite à ce modèle est la présence des posts du type \"daily thread\". En effet, ces posts ne sont pas d'un grand intérêt car le contributeur écrit chaque jour le même post. Nous allons donc réaliser des statistques descriptives après avoir supprimé tous les \"daily threads\".\n",
    "\n",
    " Pour ce qui est des commentaires, Apple apparait 2 fois en moyenne sur l'ensemble des posts tandis qu'Iphone apparait 1.5 fois en moyenne. La conclusion que nous faisons est la même que précédemment : les contributeurs discutent principalement des produits d'Apple et assez peu, voire pas du tout, de la concurrence d'Apple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant nettoyer le tableau pour l'implémenter dans l'algorithme de trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [col for col in df.columns if col.endswith('_count')]\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "cols_to_drop2 = [col for col in df.columns if col.endswith('_count2')]\n",
    "df.drop(cols_to_drop2, axis=1, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
