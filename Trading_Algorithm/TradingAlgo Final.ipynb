{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme de trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette dernière partie du projet, nous récupérons les deux modèles entraînés ainsi que les articles de presse, les posts Reddit et le cours de l'action Apple pour conseiller à notre utilisateur d'acheter ou non une action Apple avant que le marché n'ouvre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renseignez la date du jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "#On demande à l'utilisateur de saisir une date\n",
    "\n",
    "user_date_input = input(\"Entrez la date d'aujourd'hui (format AAAA-MM-JJ): \")\n",
    "current_date = datetime.strptime(user_date_input, \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rappelle les fonctions utiles, codées précédemment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Nettoyage du texte\n",
    "def cleaning_text(text):\n",
    "    #Passage du texte en miniscules\n",
    "    text=text.lower()\n",
    "    #Suppression des chiffres\n",
    "    text=re.sub(r'\\d+', '', text)\n",
    "    #Suppression de la ponctuation et des symboles spéciaux\n",
    "    text=re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Suppression des stopwords\n",
    "def clean_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_text=[word for word in words if word not in stops]\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "#Fonction de nettoyage du contenu html adaptée à la structure des articles Forbes\n",
    "def clean_html_1(text_html): \n",
    "    soup=BeautifulSoup(text_html, 'html.parser')\n",
    "    title=soup.find_all('h1', class_=True)\n",
    "    content=soup.find_all('p')\n",
    "    united_content = ' '.join(el.get_text(strip=True) for el in title + content )\n",
    "    return united_content\n",
    "\n",
    "#Fonctions de récupérations des commentaires principaux\n",
    "def extract_submission_id(url):\n",
    "    match = re.search(r'/comments/(\\w+)/', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_top_comments(url):\n",
    "    submission_id = extract_submission_id(url)\n",
    "    if not submission_id:\n",
    "        print(f\"Erreur pour '{url}'\")\n",
    "        return []\n",
    "    submission = reddit.submission(id=submission_id)\n",
    "    submission.comment_sort = 'top'\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    \n",
    "    top_comments = []\n",
    "    for comment in submission.comments[:5]:  # Prendre les 5 premiers commentaires\n",
    "        top_comments.append(comment.body)\n",
    "    \n",
    "    return top_comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des articles de presse de Forbes des 48h précédentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise à nouveau le code rédigé dans la partie PressArticles.ipynb en l'adaptant à la date rentrée par l'utilisateur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dabadieraphael/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "start_date = current_date - timedelta(days=2)\n",
    "end_date = current_date\n",
    "parameters = {\n",
    "    'q': 'Apple',\n",
    "    'domains': 'forbes.com',\n",
    "    'from': start_date.strftime(\"%Y-%m-%d\"),\n",
    "    'to': end_date.strftime(\"%Y-%m-%d\"),\n",
    "    'sortBy': 'publishedAt',\n",
    "    'apiKey': '1b20fb6f9b9d40f0b4e4ad6fe5d90755'  \n",
    "}\n",
    "\n",
    "url = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "\n",
    "response = requests.get(url, params=parameters)\n",
    "articles = response.json().get('articles', [])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On stock les articles récupérés pour l'analyse de sentiments dans un **nouveau fichier CSV** afin de permettre à l'utilisateur d'y accéder facilement et de potentiellement accéder à son contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titre</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date de publication</th>\n",
       "      <th>Description</th>\n",
       "      <th>Source</th>\n",
       "      <th>Contenu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple Execs On Continuity, The Secret Sauce Th...</td>\n",
       "      <td>https://www.forbes.com/sites/davidphelan/2023/...</td>\n",
       "      <td>2023-12-29T14:00:57Z</td>\n",
       "      <td>Apple’s products talk to each other with an in...</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;link rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple Watch Series 9, Ultra 2 Back On Sale Now...</td>\n",
       "      <td>https://www.forbes.com/sites/davidphelan/2023/...</td>\n",
       "      <td>2023-12-28T10:00:42Z</td>\n",
       "      <td>The latest twist in the Apple Watch sales ban ...</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;link rel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Titre  \\\n",
       "0  Apple Execs On Continuity, The Secret Sauce Th...   \n",
       "1  Apple Watch Series 9, Ultra 2 Back On Sale Now...   \n",
       "\n",
       "                                                 URL   Date de publication  \\\n",
       "0  https://www.forbes.com/sites/davidphelan/2023/...  2023-12-29T14:00:57Z   \n",
       "1  https://www.forbes.com/sites/davidphelan/2023/...  2023-12-28T10:00:42Z   \n",
       "\n",
       "                                         Description  Source  \\\n",
       "0  Apple’s products talk to each other with an in...  Forbes   \n",
       "1  The latest twist in the Apple Watch sales ban ...  Forbes   \n",
       "\n",
       "                                             Contenu  \n",
       "0  <!DOCTYPE html><html lang=\"en\"><head><link rel...  \n",
       "1  <!DOCTYPE html><html lang=\"en\"><head><link rel...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('articles_algo.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Titre', 'URL', 'Date de publication', 'Description','Source'])\n",
    "    for article in articles:\n",
    "        if \"Apple\" not in article['title']:\n",
    "          continue\n",
    "        writer.writerow([article['title'], article['url'], article['publishedAt'], article['description'], article['source']['name']])\n",
    "\n",
    "df=pd.read_csv('articles_algo.csv')\n",
    "\n",
    "urls = [url for url in df['URL']]\n",
    "html_contents = []\n",
    "compteur=0\n",
    "df['Contenu']=[None] * len(df)\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        html_contents.append(response.text)\n",
    "        df['Contenu'].loc[compteur]=response.text\n",
    "    else:\n",
    "        print(f\"Échec de récupération pour {url}\")\n",
    "    compteur+=1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df)>0:\n",
    "    df['Cleaned_html_content'] = df.apply(lambda row: clean_html_1(row['Contenu']), axis=1)\n",
    "    df['Content_cleaned']=df['Cleaned_html_content'].apply(cleaning_text)\n",
    "    df['Content_cleaned_from_stopwords']=df['Content_cleaned'].apply(clean_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de sentiments avec un modèle pré-entraîné"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vu dans PressArticles.ipynb, on utilise le modèle transformers qui est le plus performant pour analyser les articles récupérés. On associe à chacun d'entre eux un label \"**POSITIVE**\" ou \"**NEGATIVE**\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dabadieraphael/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titre</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date de publication</th>\n",
       "      <th>Description</th>\n",
       "      <th>Source</th>\n",
       "      <th>Contenu</th>\n",
       "      <th>Cleaned_html_content</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>Content_cleaned_from_stopwords</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple Execs On Continuity, The Secret Sauce Th...</td>\n",
       "      <td>https://www.forbes.com/sites/davidphelan/2023/...</td>\n",
       "      <td>2023-12-29T14:00:57Z</td>\n",
       "      <td>Apple’s products talk to each other with an in...</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;link rel...</td>\n",
       "      <td>Apple Execs On Continuity, The Secret Sauce Th...</td>\n",
       "      <td>apple execs on continuity the secret sauce tha...</td>\n",
       "      <td>apple execs continuity secret sauce makes ipho...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple Watch Series 9, Ultra 2 Back On Sale Now...</td>\n",
       "      <td>https://www.forbes.com/sites/davidphelan/2023/...</td>\n",
       "      <td>2023-12-28T10:00:42Z</td>\n",
       "      <td>The latest twist in the Apple Watch sales ban ...</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;link rel...</td>\n",
       "      <td>Apple Watch Series 9, Ultra 2 Back On Sale Now...</td>\n",
       "      <td>apple watch series  ultra  back on sale now in...</td>\n",
       "      <td>apple watch series ultra back sale latest surp...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Titre  \\\n",
       "0  Apple Execs On Continuity, The Secret Sauce Th...   \n",
       "1  Apple Watch Series 9, Ultra 2 Back On Sale Now...   \n",
       "\n",
       "                                                 URL   Date de publication  \\\n",
       "0  https://www.forbes.com/sites/davidphelan/2023/...  2023-12-29T14:00:57Z   \n",
       "1  https://www.forbes.com/sites/davidphelan/2023/...  2023-12-28T10:00:42Z   \n",
       "\n",
       "                                         Description  Source  \\\n",
       "0  Apple’s products talk to each other with an in...  Forbes   \n",
       "1  The latest twist in the Apple Watch sales ban ...  Forbes   \n",
       "\n",
       "                                             Contenu  \\\n",
       "0  <!DOCTYPE html><html lang=\"en\"><head><link rel...   \n",
       "1  <!DOCTYPE html><html lang=\"en\"><head><link rel...   \n",
       "\n",
       "                                Cleaned_html_content  \\\n",
       "0  Apple Execs On Continuity, The Secret Sauce Th...   \n",
       "1  Apple Watch Series 9, Ultra 2 Back On Sale Now...   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  apple execs on continuity the secret sauce tha...   \n",
       "1  apple watch series  ultra  back on sale now in...   \n",
       "\n",
       "                      Content_cleaned_from_stopwords predicted_sentiment  \n",
       "0  apple execs continuity secret sauce makes ipho...            POSITIVE  \n",
       "1  apple watch series ultra back sale latest surp...            NEGATIVE  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "sentiments=[]\n",
    "#Comme nous l'avions vu précédemment, il faut tronquer les textes pour les réduire à 512 tokens au risque d'obtenir un message d'erreur\n",
    "for i in range(len(df)):\n",
    "    text = df.iloc[i,8]\n",
    "    text = text[:512]\n",
    "    sentiment = classifier(text)[0]['label']  \n",
    "    sentiments.append(sentiment)\n",
    "df['predicted_sentiment'] = sentiments\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère le pourcentage d'articles à connotation positive parmi les articles qu'on a récupérés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage de prédictions 'POSITIVE': 50.0\n"
     ]
    }
   ],
   "source": [
    "rate_pos = (df['predicted_sentiment'] == 'POSITIVE').mean()\n",
    "percentage_pos = rate_pos * 100\n",
    "print(f\"Pourcentage de prédictions 'POSITIVE': {percentage_pos:}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des posts Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: praw in /Users/dabadieraphael/Library/Python/3.9/lib/python/site-packages (7.7.1)\n",
      "Requirement already satisfied: update-checker>=0.18 in /Users/dabadieraphael/Library/Python/3.9/lib/python/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /Users/dabadieraphael/Library/Python/3.9/lib/python/site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/dabadieraphael/Library/Python/3.9/lib/python/site-packages (from praw) (1.7.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/dabadieraphael/Library/Python/3.9/lib/python/site-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dabadieraphael/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dabadieraphael/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dabadieraphael/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dabadieraphael/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.6)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "reddit = praw.Reddit(client_id='xIq0ALkJ0RWzM5ZLwwiQKA',\n",
    "                     client_secret='DeHliktGK8nfhDXsJFiebqgeZKhHXQ',\n",
    "                     user_agent='Matlpg')\n",
    "\n",
    "start_date = current_date - timedelta(days=2)\n",
    "\n",
    "subreddit = reddit.subreddit('apple')\n",
    "top_posts = subreddit.new(limit=500)  # On récupère ainsi les 500 derniers posts Reddit sur Apple\n",
    "\n",
    "posts_data = []\n",
    "for post in top_posts:\n",
    "    post_date = datetime.utcfromtimestamp(post.created_utc)\n",
    "    if start_date <= post_date < current_date:\n",
    "        text = post.selftext if post.selftext else \"Text Not Available\"\n",
    "        post_data = {\n",
    "            \"Titre\": post.title,\n",
    "            \"Auteur\": str(post.author),\n",
    "            \"Texte\": text,\n",
    "            \"Date\": post_date,\n",
    "            \"url\": post.url\n",
    "        }\n",
    "        posts_data.append(post_data)\n",
    "\n",
    "df_reddit = pd.DataFrame(posts_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titre</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Texte</th>\n",
       "      <th>Date</th>\n",
       "      <th>url</th>\n",
       "      <th>Cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microsoft Copilot app released for iOS</td>\n",
       "      <td>parity_bits</td>\n",
       "      <td>It’s basically just the Copilot tab from the B...</td>\n",
       "      <td>2023-12-29 22:30:29</td>\n",
       "      <td>https://apps.apple.com/us/app/microsoft-copilo...</td>\n",
       "      <td>basically copilot tab bing app without bing bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daily Advice Thread - December 29, 2023</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Welcome to the Daily Advice Thread for /r/Appl...</td>\n",
       "      <td>2023-12-29 11:00:34</td>\n",
       "      <td>https://www.reddit.com/r/apple/comments/18tkm9...</td>\n",
       "      <td>welcome daily advice thread rapple thread used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Swipe on Dynamic Island (is this well known?) ...</td>\n",
       "      <td>safereddddditer175</td>\n",
       "      <td>\\nIs this a well known feature? In MKBHD’s lat...</td>\n",
       "      <td>2023-12-29 10:08:57</td>\n",
       "      <td>https://youtu.be/YmwskGLycHo?si=oKM--J62UM7SE_hH</td>\n",
       "      <td>well known feature mkbhds latest iphone pro re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Daily Advice Thread - December 28, 2023</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Welcome to the Daily Advice Thread for /r/Appl...</td>\n",
       "      <td>2023-12-28 11:00:30</td>\n",
       "      <td>https://www.reddit.com/r/apple/comments/18srcx...</td>\n",
       "      <td>welcome daily advice thread rapple thread used...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Titre              Auteur  \\\n",
       "0             Microsoft Copilot app released for iOS         parity_bits   \n",
       "2            Daily Advice Thread - December 29, 2023       AutoModerator   \n",
       "3  Swipe on Dynamic Island (is this well known?) ...  safereddddditer175   \n",
       "6            Daily Advice Thread - December 28, 2023       AutoModerator   \n",
       "\n",
       "                                               Texte                Date  \\\n",
       "0  It’s basically just the Copilot tab from the B... 2023-12-29 22:30:29   \n",
       "2  Welcome to the Daily Advice Thread for /r/Appl... 2023-12-29 11:00:34   \n",
       "3  \\nIs this a well known feature? In MKBHD’s lat... 2023-12-29 10:08:57   \n",
       "6  Welcome to the Daily Advice Thread for /r/Appl... 2023-12-28 11:00:30   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://apps.apple.com/us/app/microsoft-copilo...   \n",
       "2  https://www.reddit.com/r/apple/comments/18tkm9...   \n",
       "3   https://youtu.be/YmwskGLycHo?si=oKM--J62UM7SE_hH   \n",
       "6  https://www.reddit.com/r/apple/comments/18srcx...   \n",
       "\n",
       "                                        Cleaned_text  \n",
       "0  basically copilot tab bing app without bing bl...  \n",
       "2  welcome daily advice thread rapple thread used...  \n",
       "3  well known feature mkbhds latest iphone pro re...  \n",
       "6  welcome daily advice thread rapple thread used...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit['Cleaned_text']=df_reddit['Texte'].apply(cleaning_text)\n",
    "df_reddit['Cleaned_text']=df_reddit['Cleaned_text'].apply(clean_stopwords)\n",
    "df_reddit = df_reddit[~df_reddit['Texte'].str.contains(\"Text Not Available\")]\n",
    "df_reddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère les commentaires **les plus importants** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur pour 'https://apps.apple.com/us/app/microsoft-copilot/id6472538445'\n",
      "Erreur pour 'https://youtu.be/YmwskGLycHo?si=oKM--J62UM7SE_hH'\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_reddit.iterrows():\n",
    "    top_comments = get_top_comments(row['url'])\n",
    "    for i, comment in enumerate(top_comments):\n",
    "        df_reddit.loc[index, f'comment_{i+1}'] = comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titre</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Texte</th>\n",
       "      <th>Date</th>\n",
       "      <th>url</th>\n",
       "      <th>Cleaned_text</th>\n",
       "      <th>comment_1</th>\n",
       "      <th>comment_2</th>\n",
       "      <th>comment_3</th>\n",
       "      <th>comment_4</th>\n",
       "      <th>comment_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daily Advice Thread - December 29, 2023</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Welcome to the Daily Advice Thread for /r/Appl...</td>\n",
       "      <td>2023-12-29 11:00:34</td>\n",
       "      <td>https://www.reddit.com/r/apple/comments/18tkm9...</td>\n",
       "      <td>welcome daily advice thread rapple thread used...</td>\n",
       "      <td>I recently sent off my Apple Watch for repair ...</td>\n",
       "      <td>Guys, I fucked up, made a factory reset on my ...</td>\n",
       "      <td>I have a M1 Macbook Air 13 that I love but it'...</td>\n",
       "      <td>I’m looking into buying a Studio Display to pa...</td>\n",
       "      <td>If you invite people into Family Sharing, does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Daily Advice Thread - December 28, 2023</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Welcome to the Daily Advice Thread for /r/Appl...</td>\n",
       "      <td>2023-12-28 11:00:30</td>\n",
       "      <td>https://www.reddit.com/r/apple/comments/18srcx...</td>\n",
       "      <td>welcome daily advice thread rapple thread used...</td>\n",
       "      <td>I just impulsively upgraded from an iPhone 12 ...</td>\n",
       "      <td>I am seriously considering purchasing two Appl...</td>\n",
       "      <td>App Store Always Requiring Password\\n\\nWhy is ...</td>\n",
       "      <td>I have a files app on my iPhone that no longer...</td>\n",
       "      <td>I've read a few threads regarding this and rea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Titre         Auteur  \\\n",
       "2  Daily Advice Thread - December 29, 2023  AutoModerator   \n",
       "6  Daily Advice Thread - December 28, 2023  AutoModerator   \n",
       "\n",
       "                                               Texte                Date  \\\n",
       "2  Welcome to the Daily Advice Thread for /r/Appl... 2023-12-29 11:00:34   \n",
       "6  Welcome to the Daily Advice Thread for /r/Appl... 2023-12-28 11:00:30   \n",
       "\n",
       "                                                 url  \\\n",
       "2  https://www.reddit.com/r/apple/comments/18tkm9...   \n",
       "6  https://www.reddit.com/r/apple/comments/18srcx...   \n",
       "\n",
       "                                        Cleaned_text  \\\n",
       "2  welcome daily advice thread rapple thread used...   \n",
       "6  welcome daily advice thread rapple thread used...   \n",
       "\n",
       "                                           comment_1  \\\n",
       "2  I recently sent off my Apple Watch for repair ...   \n",
       "6  I just impulsively upgraded from an iPhone 12 ...   \n",
       "\n",
       "                                           comment_2  \\\n",
       "2  Guys, I fucked up, made a factory reset on my ...   \n",
       "6  I am seriously considering purchasing two Appl...   \n",
       "\n",
       "                                           comment_3  \\\n",
       "2  I have a M1 Macbook Air 13 that I love but it'...   \n",
       "6  App Store Always Requiring Password\\n\\nWhy is ...   \n",
       "\n",
       "                                           comment_4  \\\n",
       "2  I’m looking into buying a Studio Display to pa...   \n",
       "6  I have a files app on my iPhone that no longer...   \n",
       "\n",
       "                                           comment_5  \n",
       "2  If you invite people into Family Sharing, does...  \n",
       "6  I've read a few threads regarding this and rea...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.replace('nan', np.nan, inplace=True) #Il y avait un problème avec le formatage précédent des 'nan'\n",
    "df_reddit = df_reddit.dropna(axis=0, subset=['comment_1', 'comment_2', 'comment_3', 'comment_4', 'comment_5'])\n",
    "df_reddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nettoyage** des commentaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit['comment_1_clean'] = df_reddit['comment_1'].apply(cleaning_text)\n",
    "df_reddit['comment_2_clean'] = df_reddit['comment_2'].apply(cleaning_text)\n",
    "df_reddit['comment_3_clean'] = df_reddit['comment_3'].apply(cleaning_text)\n",
    "df_reddit['comment_4_clean'] = df_reddit['comment_4'].apply(cleaning_text)\n",
    "df_reddit['comment_5_clean'] = df_reddit['comment_5'].apply(cleaning_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de **sentiments**, une fois de plus avec transformers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transformers = pipeline(\"sentiment-analysis\")\n",
    "transformers_sentiments = []\n",
    "for index, row in df_reddit.iterrows():\n",
    "    for col in ['comment_1_clean', 'comment_2_clean', 'comment_3_clean', 'comment_4_clean', 'comment_5_clean']:\n",
    "        transformers_result = model_transformers(row[col][:512])[0] #Il faut limiter la taille du texte à 512 caractères pour ce mmodèle comme vu précédemment\n",
    "        transformers_sentiment = transformers_result['label'].lower()\n",
    "        transformers_sentiments.append(transformers_sentiment)\n",
    "\n",
    "transformers_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 0.0% de posts positifs\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "for el in transformers_sentiment:\n",
    "    if el==\"positive\":\n",
    "        pos+=1\n",
    "pos_reddit=(pos/len(df_reddit))*100\n",
    "print(f\"Il y a {pos_reddit}% de posts positifs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des informations de Yahoo Finance "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On l'a vu dans le fichier dédié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>73.152657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>72.441460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>73.018684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>72.675270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>73.844337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>193.600006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>193.050003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>193.149994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>193.580002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>192.529999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1006 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       Close\n",
       "0    2020-01-02   73.152657\n",
       "1    2020-01-03   72.441460\n",
       "2    2020-01-06   73.018684\n",
       "3    2020-01-07   72.675270\n",
       "4    2020-01-08   73.844337\n",
       "...         ...         ...\n",
       "1001 2023-12-22  193.600006\n",
       "1002 2023-12-26  193.050003\n",
       "1003 2023-12-27  193.149994\n",
       "1004 2023-12-28  193.580002\n",
       "1005 2023-12-29  192.529999\n",
       "\n",
       "[1006 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import yfinance as yf \n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "\n",
    "APL = \"AAPL\"\n",
    "data = yf.Ticker(APL) # Extraction avec la fonction yf.Ticker de yfinance\n",
    "prix_rec = data.history(period = '1d', start = '2020-1-1', end = current_date)\n",
    "prix_rec.to_csv('AAPLt.csv')\n",
    "df=pd.read_csv(\"AAPLt.csv\") \n",
    "df['Date'] = df['Date'].astype(str)\n",
    "df['Date'] = df['Date'].str.slice(0, 10)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df[['Date', 'Close']]\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction de la valeur du lendemain "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a vu dans la partie dédiée que le modèle le plus efficace était le naive forecast.\n",
    "Cependant, par définition du modèle, la valeur du lendemain que le modèle va prédire est la valeur réelle d'aujourd'hui. Donc il ne sera pas possible de tirer de conclusion sur oui ou non il faut acheter l'action, car sa valeur prédite sera la même qu'aujourd'hui."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour résoudre ce problème, nous allons comparer la valeur prédite, donc la valeur à J+1, avec la valeur à J-1.\n",
    "Si la valeur est supérieure, alors nous considérerons qu'il faut acheter. \n",
    "Sinon, il vaut mieux ne pas acheter.\n",
    "Une autre solution proposée est d'utiliser quand même le modèle LSTM, bien qu'il soit moins précis que le modèle de naive forecast, mais qui ne nécessite pas de recourir à ce compromis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur prédite du lendemain est donc la valeur de l'action au jour d'input, c'est à dire la dernière valeur de fermeture du marché :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192.52999877929688"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valeur_lendemain = df['Close'].iloc[-1]\n",
    "valeur_lendemain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparons-la à la valeur du jour J-1, qui est donc :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193.5800018310547"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valeur_prec = df['Close'].iloc[-2]\n",
    "valeur_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEGATIVE'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def acheter_ou_non():\n",
    "    if valeur_lendemain > valeur_prec :\n",
    "        return 'POSITIVE'\n",
    "    else : \n",
    "        return 'NEGATIVE'\n",
    "    \n",
    "acheter_ou_non()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passez votre tour\n"
     ]
    }
   ],
   "source": [
    "if rate_pos>0.5 and pos_reddit>50 and acheter_ou_non()=='POSITIVE':\n",
    "    print(\"C'est le moment d'acheter\")\n",
    "else :\n",
    "    print(\"Passez votre tour\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons fait le choix de ne recommander à notre utilisateur d'acheter l'action que si tous les voyants sont au vert mais nous pourrions facilement **changer les critères sélectionnés**. Nous avons montré dans ce projet qu'il était possible d'utiliser à la fois du **NLP** et de la **prédiction de séries temporelles** pour créer un algorithme de trading.\n",
    "\n",
    "Avec **plus de ressources et de données**, nous aurions pu entraîner notre **propre modèle** sur des **données plus pertinentes** et ainsi l'aider à **généraliser** aux articles et posts traités ici. Ce projet pourrait être facilement augmenté en récupérant par exemple **massivement** des données de X (anciennement Twitter) et en mettant en place une exécution en **temps réel** de l'algorithme. \n",
    "\n",
    "Ce projet sera amené à être amélioré et testé sur des données historiques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
